{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suryap19997-spec/Classroom-Analysis/blob/main/New_Classroom_Content_Analysis_End_to_End_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step - 1: üîß Batch compress all Classroom Recording MP4 files**"
      ],
      "metadata": {
        "id": "BfJMTSs4afFz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1beN-EhDaC2D"
      },
      "outputs": [],
      "source": [
        "#@title üîß Batch compress all MP4 files in \"School Project Video\" folder to ‚â§70 MB each\n",
        "import os, json, subprocess, shlex, textwrap\n",
        "\n",
        "# ===== 0) Install ffmpeg =====\n",
        "!apt -y -qq install ffmpeg >/dev/null\n",
        "\n",
        "# ===== 1) Mount Google Drive =====\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# ===== 2) Paths =====\n",
        "input_folder  = \"/content/drive/MyDrive/Colab Notebooks/Reduce Size/School Project Video\"\n",
        "output_folder = os.path.join(input_folder, \"New\")\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "target_size_mb = 70   #@param {type:\"number\"}\n",
        "max_width      = 854  #@param {type:\"number\"} # 854‚âà480p, 640‚âà360p\n",
        "audio_kbps     = 96   #@param {type:\"number\"}\n",
        "\n",
        "# ===== 3) Helpers =====\n",
        "def run(cmd):\n",
        "    p = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    if p.returncode != 0:\n",
        "        print(p.stdout)\n",
        "        raise RuntimeError(f\"Command failed: {cmd}\")\n",
        "    return p.stdout\n",
        "\n",
        "def probe_duration(path):\n",
        "    cmd = f'ffprobe -v error -show_entries format=duration -of json {shlex.quote(path)}'\n",
        "    out = run(cmd)\n",
        "    return float(json.loads(out)[\"format\"][\"duration\"])\n",
        "\n",
        "def compute_target_bitrates(target_mb, duration_s, audio_kbps=96):\n",
        "    total_kbps = (target_mb * 8 * 1024) / max(duration_s, 1e-6)\n",
        "    video_kbps = max(50, int(total_kbps - audio_kbps))\n",
        "    return video_kbps, int(audio_kbps)\n",
        "\n",
        "def human_time(s):\n",
        "    s = int(round(s))\n",
        "    h, m, s = s//3600, (s%3600)//60, s%60\n",
        "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
        "\n",
        "# ===== 4) Loop through all mp4 files =====\n",
        "for fn in sorted(os.listdir(input_folder)):\n",
        "    if not fn.lower().endswith(\".mp4\"):\n",
        "        continue\n",
        "    src = os.path.join(input_folder, fn)\n",
        "    stem, _ = os.path.splitext(fn)\n",
        "    out_path = os.path.join(output_folder, f\"{stem}_compressed.mp4\")\n",
        "\n",
        "    # Skip if already exists\n",
        "    if os.path.exists(out_path):\n",
        "        print(f\"‚è≠Ô∏è Skipping {fn} (already compressed)\")\n",
        "        continue\n",
        "\n",
        "    # Probe and compute bitrate\n",
        "    dur = probe_duration(src)\n",
        "    v_kbps, a_kbps = compute_target_bitrates(target_size_mb, dur, audio_kbps)\n",
        "    print(f\"\\n‚ñ∂ {fn} | {human_time(dur)} | v~{v_kbps}k + a{a_kbps}k\")\n",
        "\n",
        "    scale_filter = f\"scale='min({int(max_width)},iw)':-2\"\n",
        "    passlog = \"/content/ffmpeg2pass\"\n",
        "\n",
        "    # Pass 1\n",
        "    cmd1 = f\"\"\"\n",
        "    ffmpeg -y -hide_banner -loglevel error -i {shlex.quote(src)} -vf {shlex.quote(scale_filter)} \\\n",
        "      -c:v libx264 -preset medium -b:v {v_kbps}k -pass 1 -passlogfile {shlex.quote(passlog)} \\\n",
        "      -an -movflags +faststart -f mp4 /dev/null\n",
        "    \"\"\"\n",
        "    # Pass 2\n",
        "    cmd2 = f\"\"\"\n",
        "    ffmpeg -y -hide_banner -loglevel error -i {shlex.quote(src)} -vf {shlex.quote(scale_filter)} \\\n",
        "      -c:v libx264 -preset medium -b:v {v_kbps}k -pass 2 -passlogfile {shlex.quote(passlog)} \\\n",
        "      -c:a aac -b:a {a_kbps}k -movflags +faststart {shlex.quote(out_path)}\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"‚Äî Pass 1 ‚Äî\")\n",
        "    run(textwrap.dedent(cmd1))\n",
        "    print(\"‚Äî Pass 2 ‚Äî\")\n",
        "    run(textwrap.dedent(cmd2))\n",
        "\n",
        "    # Clean logs\n",
        "    for ext in (\".log\", \".log.mbtree\"):\n",
        "        p = f\"{passlog}{ext}\"\n",
        "        if os.path.exists(p):\n",
        "            os.remove(p)\n",
        "\n",
        "    final_mb = os.path.getsize(out_path)/(1024*1024)\n",
        "    print(f\"   ‚úÖ Done: {out_path} ({final_mb:.2f} MB)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2: Creating audio file for each compressed video for Script Analysis**"
      ],
      "metadata": {
        "id": "BtkXU0ugdEWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A) Create Excel sheet with video path and its corresponding audio path on drive (mp3)"
      ],
      "metadata": {
        "id": "oR8BVy6YbLcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "# Base folder\n",
        "video_folder = '/content/drive/MyDrive/Colab Notebooks/Reduce Size/School Project Video/New/'\n",
        "audio_subfolder = os.path.join(video_folder, 'audio_files')\n",
        "\n",
        "# Step 1: Create audio_files folder if it doesn't exist\n",
        "os.makedirs(audio_subfolder, exist_ok=True)\n",
        "\n",
        "# Step 2: Get only .mp4 and .mp3 files in the main folder\n",
        "target_files = [f for f in os.listdir(video_folder)\n",
        "                if os.path.isfile(os.path.join(video_folder, f)) and f.lower().endswith(('.mp4', '.mp3'))]\n",
        "\n",
        "# Step 3: Copy .mp3 files to audio_files folder\n",
        "for file in target_files:\n",
        "    if file.lower().endswith('.mp3'):\n",
        "        src = os.path.join(video_folder, file)\n",
        "        dst = os.path.join(audio_subfolder, file)\n",
        "        if not os.path.exists(dst):\n",
        "            shutil.copy(src, dst)\n",
        "            print(f\"üìÅ Copied: {file} ‚Üí audio_files/\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Skipped (already exists): {file}\")\n",
        "\n",
        "# Step 4: Create records with video_path and audio_path\n",
        "records = []\n",
        "for file in target_files:\n",
        "    full_path = os.path.join(video_folder, file)\n",
        "    audio_file_name = file.replace('.MP4', '.mp3').replace('.mp4', '.mp3')\n",
        "    audio_path = os.path.join(audio_subfolder, audio_file_name)\n",
        "    records.append({\n",
        "        'video_path': full_path,\n",
        "        'audio_path': audio_path\n",
        "    })\n",
        "\n",
        "# Step 5: Save to Excel\n",
        "df = pd.DataFrame(records)\n",
        "excel_path = os.path.join(video_folder, 'video_file_list_with_links.xlsx')\n",
        "df.to_excel(excel_path, index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Excel saved at: {excel_path}\")\n",
        "print(f\"‚úÖ Audio folder ensured at: {audio_subfolder}\")\n"
      ],
      "metadata": {
        "id": "tLPIzM86bR2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "B) Creating audio file (mp3) from compressed video and saving in above path"
      ],
      "metadata": {
        "id": "-0RC7G-mbx6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update -qq\n",
        "!apt-get install -y ffmpeg\n",
        "\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "\n",
        "# Load the Excel with video and audio paths\n",
        "excel_path = '/content/drive/MyDrive/Colab Notebooks/Reduce Size/School Project Video/New/video_file_list_with_links.xlsx'\n",
        "df = pd.read_excel(excel_path)\n",
        "\n",
        "# Loop through each row and run ffmpeg\n",
        "for index, row in df.iterrows():\n",
        "    video_path = row['video_path']\n",
        "    output_audio_path = row['audio_path']\n",
        "\n",
        "    # Skip if video_path is already an .mp3 file\n",
        "    if str(video_path).lower().endswith('.mp3'):\n",
        "        print(f\"‚è≠Ô∏è Skipping already-audio file: {video_path}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"üé¨ Extracting audio from: {video_path}\")\n",
        "\n",
        "    # Run ffmpeg command\n",
        "    command = f'ffmpeg -i \"{video_path}\" -vn -acodec libmp3lame -y \"{output_audio_path}\"'\n",
        "    subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "    print(f\"‚úÖ Saved audio to: {output_audio_path}\")\n"
      ],
      "metadata": {
        "id": "_2VRRbj8b2U0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C) Creating a excel wih video-audio path-link (google drive)"
      ],
      "metadata": {
        "id": "xS98H6gvdo5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from google.auth import default\n",
        "\n",
        "# === Step 1: Google Drive Auth ===\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "drive_service = build('drive', 'v3', credentials=creds)\n",
        "\n",
        "# === Step 2: Load Excel with video/audio paths ===\n",
        "excel_path = '/content/drive/MyDrive/Colab Notebooks/Reduce Size/School Project Video/New/video_file_list_with_links.xlsx'\n",
        "df = pd.read_excel(excel_path)\n",
        "\n",
        "# === Step 3: Get Shareable Link of Existing File in Drive ===\n",
        "def get_existing_file_link(file_path):\n",
        "    file_name = os.path.basename(file_path)\n",
        "\n",
        "    # Search for file by exact name\n",
        "    query = f\"name = '{file_name}' and trashed = false\"\n",
        "    results = drive_service.files().list(\n",
        "        q=query,\n",
        "        fields=\"files(id, name, parents)\",\n",
        "        spaces='drive'\n",
        "    ).execute()\n",
        "\n",
        "    files = results.get('files', [])\n",
        "    if not files:\n",
        "        print(f\"‚ùå File not found in Drive: {file_name}\")\n",
        "        return \"\"\n",
        "\n",
        "    file_id = files[0]['id']\n",
        "\n",
        "    # Make file public\n",
        "    try:\n",
        "        drive_service.permissions().create(\n",
        "            fileId=file_id,\n",
        "            body={'role': 'reader', 'type': 'anyone'},\n",
        "            fields='id'\n",
        "        ).execute()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Permission error for {file_name}: {e}\")\n",
        "\n",
        "    return f\"https://drive.google.com/file/d/{file_id}/view?usp=sharing\"\n",
        "\n",
        "# === Step 4: Loop through rows and create links ===\n",
        "video_links = []\n",
        "audio_links = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    video_path = row['video_path']\n",
        "    audio_path = row['audio_path']\n",
        "\n",
        "    print(f\"\\nüîó Processing: {os.path.basename(video_path)}\")\n",
        "    video_link = get_existing_file_link(video_path)\n",
        "    audio_link = get_existing_file_link(audio_path)\n",
        "\n",
        "    video_links.append(video_link)\n",
        "    audio_links.append(audio_link)\n",
        "\n",
        "# === Step 5: Update and Save Excel ===\n",
        "df['video_link'] = video_links\n",
        "df['audio_link'] = audio_links\n",
        "\n",
        "df.to_excel(excel_path, index=False)\n",
        "print(f\"\\n‚úÖ Excel updated with links: {excel_path}\")\n"
      ],
      "metadata": {
        "id": "-WyJ_Xtob41F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Convert Audio to Script with diarization**"
      ],
      "metadata": {
        "id": "lFyQqjQrd2Rd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Excel list of audio files ‚Üí converted audio ‚Üí chunks & sends to Gemini for transcription and speaker labeling ‚Üí translates English output to Hindi"
      ],
      "metadata": {
        "id": "F_H6qbbPgdhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================== Minimal Installs ========================\n",
        "# Use system ffmpeg; do not upgrade pandas/requests pinned by Colab.\n",
        "!apt-get -qq update >/dev/null\n",
        "!apt-get -qq install -y ffmpeg >/dev/null\n",
        "!pip -q install openpyxl  # for Excel I/O (keeps Colab's pandas version)\n",
        "\n",
        "# ============================ Imports ============================\n",
        "import os, io, json, time, math, wave, contextlib, subprocess, shlex, base64, requests\n",
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError\n",
        "from google.colab import drive\n",
        "\n",
        "# =========================== Mount Drive =========================\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "# =========================== Configuration ======================\n",
        "# Excel columns expected: video_path, audio_path, video_link, audio_link\n",
        "excel_path = \"/content/drive/MyDrive/Colab Notebooks/Reduce Size/School Project Video/New/video_file_list_with_links.xlsx\"\n",
        "output_excel_path = \"/content/drive/MyDrive/Colab Notebooks/Reduce Size/School Project Video/New/All_Captions_Excel.xlsx\"\n",
        "\n",
        "GEMINI_API_KEY = \"AIzaSyAncHidRLxZpPXhCTsHFIrbSz6BcZhVi_o\"   # <-- put your API key (rotate if you exposed one earlier)\n",
        "\n",
        "# REST model to use for chunk transcription/diarization and translation\n",
        "MODEL_CHUNK = \"gemini-1.5-pro\"\n",
        "\n",
        "# Chunking & runtime knobs\n",
        "LONG_AUDIO_THRESH_SEC = 90         # split if audio longer than this\n",
        "CHUNK_SEC = 60                     # chunk length (sec)\n",
        "PER_CHUNK_TIMEOUT = 150            # timeout per chunk REST call (sec)\n",
        "RETRIES_PER_CHUNK = 2              # retries per chunk\n",
        "ITEM_WATCHDOG_SEC = 540            # max time per file (sec)\n",
        "SAVE_EVERY_N_ROWS = 1              # save progress after each processed row\n",
        "DO_HINDI_TRANSLATION = True        # set False if you don't need Hindi column\n",
        "\n",
        "SUPPORTED_LANGUAGES = {\n",
        "    \"hi\": \"Hindi\", \"mr\": \"Marathi\", \"pa\": \"Punjabi\", \"en\": \"English\", \"bn\": \"Bengali\"\n",
        "}\n",
        "\n",
        "# ============================== Helpers ==========================\n",
        "def format_ts(seconds):\n",
        "    \"\"\"Format seconds as [MM:SS.mmm].\"\"\"\n",
        "    try:\n",
        "        s = float(seconds)\n",
        "    except Exception:\n",
        "        return \"[00:00.000]\"\n",
        "    m, s = divmod(s, 60.0)\n",
        "    return f\"[{int(m):02d}:{s:06.3f}]\"\n",
        "\n",
        "def strip_code_fences(s: str) -> str:\n",
        "    s = (s or \"\").strip()\n",
        "    if s.startswith(\"```\"):\n",
        "        s = s[3:]\n",
        "        if \"\\n\" in s: s = s.split(\"\\n\", 1)[1]\n",
        "    if s.endswith(\"```\"):\n",
        "        s = s[:-3]\n",
        "    return s.strip()\n",
        "\n",
        "def ensure_wav_16k_mono(in_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert any input to 16 kHz mono PCM WAV via ffmpeg (no moviepy).\n",
        "    Returns path to /content/tmp_audio_16k_mono.wav or original on failure.\n",
        "    \"\"\"\n",
        "    out_path = \"/content/tmp_audio_16k_mono.wav\"\n",
        "    try:\n",
        "        cmd = f'ffmpeg -y -i {shlex.quote(in_path)} -vn -ac 1 -ar 16000 -acodec pcm_s16le {shlex.quote(out_path)}'\n",
        "        r = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        if r.returncode != 0:\n",
        "            print(\"‚ö†Ô∏è ffmpeg convert failed:\", r.stderr.decode(errors=\"ignore\")[:300])\n",
        "            return in_path\n",
        "        return out_path\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è ffmpeg convert error: {e}\")\n",
        "        return in_path\n",
        "\n",
        "def wav_duration_sec(path: str) -> float:\n",
        "    with contextlib.closing(wave.open(path, 'rb')) as w:\n",
        "        frames = w.getnframes()\n",
        "        rate = w.getframerate() or 16000\n",
        "        return frames / float(rate)\n",
        "\n",
        "def read_wav_bytes(path: str, start_sec: float = 0.0, end_sec: float = None) -> bytes:\n",
        "    \"\"\"Return a WAV slice [start_sec, end_sec) as bytes with a proper header.\"\"\"\n",
        "    with wave.open(path, 'rb') as w:\n",
        "        n_channels = w.getnchannels()\n",
        "        sampwidth = w.getsampwidth()\n",
        "        framerate = w.getframerate()\n",
        "        nframes = w.getnframes()\n",
        "        start_frame = max(0, min(int((start_sec or 0.0) * framerate), nframes))\n",
        "        end_frame = nframes if end_sec is None else max(start_frame, min(int(end_sec * framerate), nframes))\n",
        "        w.setpos(start_frame)\n",
        "        raw = w.readframes(end_frame - start_frame)\n",
        "    buf = io.BytesIO()\n",
        "    with wave.open(buf, 'wb') as out:\n",
        "        out.setnchannels(n_channels); out.setsampwidth(sampwidth); out.setframerate(framerate)\n",
        "        out.writeframes(raw)\n",
        "    return buf.getvalue()\n",
        "\n",
        "# ---------- REST helpers ----------\n",
        "def rest_generate_text(prompt: str, *, model=MODEL_CHUNK, api_key=GEMINI_API_KEY, timeout=120) -> str:\n",
        "    \"\"\"Simple REST text generation (used for translation).\"\"\"\n",
        "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={api_key}\"\n",
        "    body = {\"contents\":[{\"parts\":[{\"text\": prompt}]}]}\n",
        "    r = requests.post(url, json=body, timeout=timeout, proxies={\"http\": None, \"https\": None})\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "    return data[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"].strip()\n",
        "\n",
        "def call_gemini_chunk(audio_wav_bytes: bytes,\n",
        "                      timeout_sec: int = PER_CHUNK_TIMEOUT,\n",
        "                      model: str = MODEL_CHUNK,\n",
        "                      api_key: str = GEMINI_API_KEY) -> dict:\n",
        "    \"\"\"\n",
        "    REST version: one chunk -> JSON (speaker-labeled, English-only).\n",
        "    Uses inline_data (base64) and disables proxies explicitly to avoid localhost routing.\n",
        "    \"\"\"\n",
        "    prompt = \"\"\"\n",
        "Transcribe and translate all speech to English.\n",
        "Diarize with these speaker labels ONLY (pick best match or \"Unknown\"):\n",
        "- \"Male Kid\"\n",
        "- \"Female Kid\"\n",
        "- \"Male Teacher (nearby)\"\n",
        "- \"Teacher (on laptop)\"\n",
        "- \"Unknown\"\n",
        "\n",
        "Split into sentence-level segments.\n",
        "Each segment JSON must be:\n",
        "{\"start\": number, \"end\": number, \"speaker\": \"Male Kid|Female Kid|Male Teacher (nearby)|Teacher (on laptop)|Unknown\", \"text_en\": \"...\"}\n",
        "\n",
        "Return ONLY JSON exactly:\n",
        "{\n",
        "  \"detected_language_code\": \"en|hi|mr|pa|bn\",\n",
        "  \"segments\": [\n",
        "    {\"start\": 0.00, \"end\": 1.23, \"speaker\": \"Male Kid\", \"text_en\": \"Hello ...\"}\n",
        "  ]\n",
        "}\n",
        "\"\"\".strip()\n",
        "\n",
        "    url = f\"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={api_key}\"\n",
        "    body = {\n",
        "        \"contents\": [{\n",
        "            \"parts\": [\n",
        "                {\"text\": prompt},\n",
        "                {\"inline_data\": {\n",
        "                    \"mime_type\": \"audio/wav\",\n",
        "                    \"data\": base64.b64encode(audio_wav_bytes).decode(\"ascii\")\n",
        "                }}\n",
        "            ]\n",
        "        }]\n",
        "    }\n",
        "    r = requests.post(url, json=body, timeout=timeout_sec, proxies={\"http\": None, \"https\": None})\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "\n",
        "    try:\n",
        "        txt = data[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "    except Exception:\n",
        "        raise RuntimeError(f\"Bad REST response: {str(data)[:500]}\")\n",
        "\n",
        "    txt = strip_code_fences(txt)\n",
        "    obj = json.loads(txt)\n",
        "    if \"segments\" not in obj:\n",
        "        raise ValueError(\"JSON missing 'segments'.\")\n",
        "    if \"detected_language_code\" not in obj:\n",
        "        obj[\"detected_language_code\"] = \"en\"\n",
        "    return obj\n",
        "\n",
        "# ---------- Main transcription (chunked, REST only) ----------\n",
        "def gemini_transcribe_with_speakers(\n",
        "    audio_path: str,\n",
        "    long_audio_thresh_sec: int = LONG_AUDIO_THRESH_SEC,\n",
        "    chunk_sec: int = CHUNK_SEC,\n",
        "    per_chunk_timeout: int = PER_CHUNK_TIMEOUT,\n",
        "    retries: int = RETRIES_PER_CHUNK,\n",
        "    sleep_sec: int = 2\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Transcribe + diarize with chunking (inline bytes, REST).\n",
        "    Returns:\n",
        "      {\"detected_language_code\": str,\n",
        "       \"segments\": [{\"start\": float, \"end\": float, \"speaker\": str, \"text_en\": str}, ...]}\n",
        "    \"\"\"\n",
        "    wav = ensure_wav_16k_mono(audio_path)\n",
        "    dur = wav_duration_sec(wav)\n",
        "    print(f\"   ‚è±Ô∏è Audio duration ‚âà {dur:.1f}s\")\n",
        "\n",
        "    def run_one(start_t: float, end_t: float):\n",
        "        b = read_wav_bytes(wav, start_t, end_t)\n",
        "        last_err = None\n",
        "        for attempt in range(1, retries + 1):\n",
        "            try:\n",
        "                print(f\"      ‚ñ∂ chunk {start_t:.1f}‚Äì{end_t:.1f}s (attempt {attempt})\")\n",
        "                return call_gemini_chunk(b, timeout_sec=per_chunk_timeout)\n",
        "            except Exception as exc:\n",
        "                last_err = exc\n",
        "                print(f\"         ‚ö†Ô∏è chunk failed: {exc}\")\n",
        "                time.sleep(sleep_sec)\n",
        "        raise last_err\n",
        "\n",
        "    # Short audio ‚Üí single call\n",
        "    if dur <= long_audio_thresh_sec:\n",
        "        return run_one(0.0, dur)\n",
        "\n",
        "    # Long audio ‚Üí chunked\n",
        "    print(\"   ‚úÇÔ∏è Long audio detected ‚Üí chunking.\")\n",
        "    offsets = []\n",
        "    s = 0.0\n",
        "    while s < dur:\n",
        "        e = min(s + chunk_sec, dur)\n",
        "        offsets.append((s, e))\n",
        "        s = e\n",
        "\n",
        "    results = []\n",
        "    for (start_t, end_t) in offsets:\n",
        "        try:\n",
        "            d = run_one(start_t, end_t)\n",
        "            # shift segment times by chunk offset (start_t)\n",
        "            for seg in d.get(\"segments\", []):\n",
        "                seg[\"start\"] = float(seg.get(\"start\", 0.0)) + start_t\n",
        "                seg[\"end\"]   = float(seg.get(\"end\",   0.0)) + start_t\n",
        "            results.append(d)\n",
        "        except Exception as exc:\n",
        "            print(f\"         ‚ùå skipping chunk {start_t:.1f}‚Äì{end_t:.1f}: {exc}\")\n",
        "\n",
        "    # Merge segments & language (majority vote)\n",
        "    segments, langs = [], []\n",
        "    for d in results:\n",
        "        segments.extend(d.get(\"segments\", []))\n",
        "        langs.append((d.get(\"detected_language_code\") or \"en\").lower())\n",
        "    segments.sort(key=lambda x: float(x.get(\"start\", 0.0)))\n",
        "\n",
        "    if not langs:\n",
        "        lang = \"en\"\n",
        "    else:\n",
        "        from collections import Counter\n",
        "        lang = Counter(langs).most_common(1)[0][0]\n",
        "\n",
        "    return {\"detected_language_code\": lang, \"segments\": segments}\n",
        "\n",
        "def translate_text_rest(text: str, target_lang: str = \"hi\") -> str:\n",
        "    \"\"\"Translate final English transcript to a target language (REST).\"\"\"\n",
        "    if not text.strip():\n",
        "        return \"\"\n",
        "    prompt = f\"\"\"Translate the following educational script to {target_lang.upper()}.\n",
        "Preserve meaning and proper nouns. Avoid abusive words. Keep it suitable for parents and children.\n",
        "\n",
        "{text}\"\"\"\n",
        "    try:\n",
        "        return rest_generate_text(prompt, model=MODEL_CHUNK, api_key=GEMINI_API_KEY, timeout=120)\n",
        "    except Exception as exc:\n",
        "        print(f\"‚ö†Ô∏è Translation failed: {exc}\")\n",
        "        return \"\"\n",
        "\n",
        "def build_linewise_english(data):\n",
        "    \"\"\"Build line-wise English transcript with fixed speaker labels.\"\"\"\n",
        "    code = (data.get(\"detected_language_code\") or \"en\").lower()\n",
        "    lang_name = SUPPORTED_LANGUAGES.get(code, code.upper())\n",
        "    lines = []\n",
        "    for seg in data.get(\"segments\", []):\n",
        "        try:\n",
        "            start = float(seg.get(\"start\", 0.0))\n",
        "            speaker = str(seg.get(\"speaker\", \"Unknown\")).strip() or \"Unknown\"\n",
        "            text_en = (seg.get(\"text_en\") or \"\").strip()\n",
        "            if text_en:\n",
        "                lines.append(f\"{format_ts(start)} {speaker}: {text_en}\")\n",
        "        except Exception:\n",
        "            continue\n",
        "    english_text = \"\\n\".join(lines)\n",
        "    return lang_name, english_text\n",
        "\n",
        "# ======================== Load Input Excel =======================\n",
        "df = pd.read_excel(excel_path)\n",
        "output_rows = []\n",
        "\n",
        "# ================== Process Rows (watchdog + saves) =============\n",
        "for idx, row in df.iterrows():\n",
        "    try:\n",
        "        video_path = row.get(\"video_path\", \"\")\n",
        "        audio_path = row.get(\"audio_path\", \"\")\n",
        "        video_link = row.get(\"video_link\", \"\")\n",
        "        audio_link = row.get(\"audio_link\", \"\")\n",
        "\n",
        "        if not audio_path or not os.path.exists(audio_path):\n",
        "            print(f\"\\n‚ö†Ô∏è Skipping (no audio): {os.path.basename(video_path or 'unknown_video')}\")\n",
        "            continue\n",
        "\n",
        "        label = os.path.basename(video_path or audio_path)\n",
        "        print(f\"\\nüéß [{idx+1}/{len(df)}] Processing (chunk-wise diarization): {label}\")\n",
        "\n",
        "        # Per-item watchdog to prevent hangs\n",
        "        data = None\n",
        "        try:\n",
        "            with ThreadPoolExecutor(max_workers=1) as ex:\n",
        "                fut = ex.submit(gemini_transcribe_with_speakers, audio_path)\n",
        "                data = fut.result(timeout=ITEM_WATCHDOG_SEC)\n",
        "        except FuturesTimeoutError:\n",
        "            print(f\"   ‚è±Ô∏è Item timeout after {ITEM_WATCHDOG_SEC}s\")\n",
        "            data = None\n",
        "\n",
        "        if not data:\n",
        "            output_rows.append({\n",
        "                \"video_path\": video_path, \"audio_path\": audio_path,\n",
        "                \"video_link\": video_link, \"audio_link\": audio_link,\n",
        "                \"detected_language\": \"Unknown\",\n",
        "                \"cleaned_transcript\": \"\", \"hindi_translated\": \"\",\n",
        "                \"english_translated\": \"\", \"error\": \"Item timeout/failure\"\n",
        "            })\n",
        "            pd.DataFrame(output_rows).to_excel(output_excel_path, index=False)\n",
        "            print(\"   ‚ùå Skipped (progress saved).\")\n",
        "            continue\n",
        "\n",
        "        lang_name, english_text = build_linewise_english(data)\n",
        "        hindi_text = translate_text_rest(english_text, target_lang=\"hi\") if DO_HINDI_TRANSLATION else \"\"\n",
        "\n",
        "        output_rows.append({\n",
        "            \"video_path\": video_path, \"audio_path\": audio_path,\n",
        "            \"video_link\": video_link, \"audio_link\": audio_link,\n",
        "            \"detected_language\": lang_name,\n",
        "            \"cleaned_transcript\": english_text,   # English, line-wise, speaker-labeled\n",
        "            \"hindi_translated\": hindi_text,       # optional\n",
        "            \"english_translated\": english_text\n",
        "        })\n",
        "\n",
        "        # Save progress incrementally\n",
        "        if SAVE_EVERY_N_ROWS and (len(output_rows) % SAVE_EVERY_N_ROWS == 0):\n",
        "            pd.DataFrame(output_rows).to_excel(output_excel_path, index=False)\n",
        "            print(f\"   üíæ Progress saved ‚Üí {output_excel_path}\")\n",
        "\n",
        "        # Clean temp wav\n",
        "        tmp_wav = \"/content/tmp_audio_16k_mono.wav\"\n",
        "        if os.path.exists(tmp_wav):\n",
        "            try: os.remove(tmp_wav)\n",
        "            except: pass\n",
        "\n",
        "        print(\"   ‚úÖ Row completed.\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nüõë Interrupted ‚Äî saving partial results‚Ä¶\")\n",
        "        pd.DataFrame(output_rows).to_excel(output_excel_path, index=False)\n",
        "        raise\n",
        "    except Exception as exc:\n",
        "        print(f\"‚ùå Unexpected error on row {idx}: {exc}\")\n",
        "        pd.DataFrame(output_rows).to_excel(output_excel_path, index=False)\n",
        "        continue\n",
        "\n",
        "# =========================== Final Save ==========================\n",
        "pd.DataFrame(output_rows).to_excel(output_excel_path, index=False)\n",
        "print(f\"\\nüìÅ All results saved to: {output_excel_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "SyNxXuaFd2iR",
        "outputId": "584bf646-818e-4d0e-c6a9-48ea7b003b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 326)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m326\u001b[0m\n\u001b[0;31m    lang_name, english_text = build_linewise_english(data)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Data Analysis**"
      ],
      "metadata": {
        "id": "lcZ7KkHcjddm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# i) Data Preparation:"
      ],
      "metadata": {
        "id": "DRC5Fpr9vMil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform Script in Readable format\n",
        "\n",
        "A) Add time stamp. speaker, linewise utterance columns"
      ],
      "metadata": {
        "id": "SQfuF5_xsAu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "INPUT_FILE = \"AllExcelCaptionData.xlsx\"        # Sheet containing all chapters\n",
        "OUTPUT_FILE = \"Linewise_Transcript.xlsx\"\n",
        "SHEET_NAME = \"Sheet1\"                       # Sheet index or name\n",
        "\n",
        "# --- STEP 1: READ ALL CHAPTERS ---\n",
        "df = pd.read_excel(INPUT_FILE, sheet_name=SHEET_NAME)\n",
        "\n",
        "all_rows = []\n",
        "\n",
        "# --- STEP 2: LOOP THROUGH EACH CHAPTER ---\n",
        "for i, row in df.iterrows():\n",
        "    video_path = row.get(\"video_path\", \"\")\n",
        "    audio_path = row.get(\"audio_path\", \"\")\n",
        "    video_link = row.get(\"video_link\", \"\")\n",
        "    audio_link = row.get(\"audio_link\", \"\")\n",
        "    detected_language = row.get(\"detected_language\", \"Hindi\")\n",
        "    transcript = str(row.get(\"transcript_text\", \"\"))\n",
        "\n",
        "    # --- STEP 3: EXTRACT timestamp, speaker, utterance ---\n",
        "    pattern = r\"\\[(\\d{2}:\\d{2}(?:\\.\\d{3})?)\\]\\s*([^:]+):\\s*(.*?)(?=\\s*\\[\\d{2}:\\d{2}\\.\\d{3}\\]|$)\"\n",
        "    matches = re.findall(pattern, transcript, re.DOTALL)\n",
        "\n",
        "    for match in matches:\n",
        "        timestamp, speaker, utterance = match\n",
        "        all_rows.append({\n",
        "            \"chapter_id\": row.get(\"chapter_id\", i+1),\n",
        "            \"video_path\": video_path,\n",
        "            \"audio_path\": audio_path,\n",
        "            \"video_link\": video_link,\n",
        "            \"audio_link\": audio_link,\n",
        "            \"detected_language\": detected_language,\n",
        "            \"timestamp\": timestamp.strip(),\n",
        "            \"speaker\": speaker.strip(),\n",
        "            \"utterance\": utterance.strip()\n",
        "        })\n",
        "\n",
        "# --- STEP 4: CREATE OUTPUT DATAFRAME ---\n",
        "final_df = pd.DataFrame(all_rows)\n",
        "\n",
        "# --- STEP 5: SAVE OUTPUT ---\n",
        "final_df.to_excel(OUTPUT_FILE, index=False)\n",
        "print(f\"‚úÖ Processed {len(df)} chapters and {len(final_df)} lines saved to '{OUTPUT_FILE}'.\")\n"
      ],
      "metadata": {
        "id": "oN5wwnDgr_pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "B) Add end_time, word count, duration column for each utterance (caption) in data"
      ],
      "metadata": {
        "id": "lZW1Zhh_sAEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import timedelta\n",
        "\n",
        "# STEP 2: Helper - parse timestamp (MM:SS.SSS ‚Üí timedelta)\n",
        "def parse_timestamp(ts_str):\n",
        "    \"\"\"Convert MM:SS.SSS to timedelta\"\"\"\n",
        "    minutes, sec_ms = ts_str.split(\":\")\n",
        "    seconds, millis = sec_ms.split(\".\")\n",
        "    return timedelta(minutes=int(minutes),\n",
        "                     seconds=int(seconds),\n",
        "                     milliseconds=int(millis))\n",
        "\n",
        "# STEP 3: Helper - format timedelta back to MM:SS.SSS\n",
        "def format_timestamp(td):\n",
        "    total_ms = int(td.total_seconds() * 1000)\n",
        "    minutes = total_ms // 60000\n",
        "    seconds = (total_ms % 60000) // 1000\n",
        "    millis = total_ms % 1000\n",
        "    return f\"{minutes:02}:{seconds:02}.{millis:03}\"\n",
        "\n",
        "# STEP 4: Main function\n",
        "def add_end_time(file_path, output_dir, words_per_second=2.2):\n",
        "    # Read Excel\n",
        "    df = pd.read_excel(file_path)\n",
        "\n",
        "    # Convert timestamp to timedelta\n",
        "    df['start_time_td'] = df['timestamp'].astype(str).apply(parse_timestamp)\n",
        "\n",
        "    # Word count\n",
        "    df['word_count'] = df['utterance'].astype(str).apply(lambda x: len(x.split()))\n",
        "\n",
        "    # Duration estimate\n",
        "    df['duration_sec'] = df['word_count'] / words_per_second\n",
        "\n",
        "    # Approx end time\n",
        "    df['end_time_td'] = df['start_time_td'] + df['duration_sec'].apply(lambda x: timedelta(seconds=x))\n",
        "\n",
        "    # Clip if end_time > next start_time\n",
        "    for i in range(len(df) - 1):\n",
        "        next_start = df.loc[i+1, 'start_time_td']\n",
        "        if df.loc[i, 'end_time_td'] > next_start:\n",
        "            df.loc[i, 'end_time_td'] = next_start\n",
        "\n",
        "    # Convert back to MM:SS.SSS format\n",
        "    df['end_time'] = df['end_time_td'].apply(format_timestamp)\n",
        "\n",
        "    # Drop helper cols if not needed\n",
        "    df_out = df.drop(columns=['start_time_td', 'end_time_td'])\n",
        "\n",
        "    # Save with same name into output_dir\n",
        "    filename = os.path.basename(file_path)\n",
        "    output_path = os.path.join(output_dir, filename)\n",
        "    df_out.to_excel(output_path, index=False)\n",
        "\n",
        "    print(f\"‚úÖ File saved to: {output_path}\")\n",
        "    return df_out\n",
        "\n",
        "# STEP 5: Run the function\n",
        "# Example usage:\n",
        "# Place your input file in the target Drive folder or adjust path below.\n",
        "input_path = \"/content/drive/MyDrive/Colab Notebooks/School Project_Reduce Size_Final Code/Data Analysis/Chapter Data Ready for Analysis.xlsx\"\n",
        "output_dir = \"/content/drive/MyDrive/Colab Notebooks/School Project_Reduce Size_Final Code/Data Analysis/Data Transformation\"\n",
        "\n",
        "processed_df = add_end_time(input_path, output_dir)\n",
        "processed_df.head()\n"
      ],
      "metadata": {
        "id": "PM9aulkMjdy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C) Adding \"is_question\" column"
      ],
      "metadata": {
        "id": "AfnQLWwUwM6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 1: Using AI API for identifying question"
      ],
      "metadata": {
        "id": "OtACI0j3whPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#testing Perplexity API\n",
        "import requests\n",
        "\n",
        "api_key = \"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\"\n",
        "\n",
        "url = \"https://api.perplexity.ai/chat/completions\"\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {api_key}\",\n",
        "    \"Content-Type\": \"application/json\",\n",
        "}\n",
        "data = {\n",
        "    \"model\": \"sonar-pro\",  # Insert the correct model id\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello, are you working?\"}]\n",
        "}\n",
        "\n",
        "\n",
        "try:\n",
        "    response = requests.post(url, json=data, headers=headers)\n",
        "    print(\"Status code:\", response.status_code)\n",
        "    print(\"Content:\", response.text)\n",
        "    if response.status_code == 200:\n",
        "        print(\"Perplexity API is working!\")\n",
        "    else:\n",
        "        print(\"Check your API key or parameters.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(\"Error:\", e)\n"
      ],
      "metadata": {
        "id": "0h3UcEuXwliT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Question Detector with Perplexity API"
      ],
      "metadata": {
        "id": "t-NF_AI6xJ8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# üîπ Question Detector with Perplexity API\n",
        "# üîπ Save output to a chosen Drive folder\n",
        "# ===============================\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload\n",
        "import io\n",
        "import datetime\n",
        "\n",
        "# Authenticate Google Drive\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "# ===============================\n",
        "# üîπ Configuration\n",
        "# ===============================\n",
        "PERPLEXITY_API_KEY = \"pplx-ih7VJ9rxAc8qXZhfdYyJ2Xu7fDU5AGnc32xpyUXLXXXX\"   # ‚Üê replace with your key\n",
        "PERPLEXITY_URL = \"https://api.perplexity.ai/chat/completions\"\n",
        "\n",
        "# ===============================\n",
        "# üîπ Helper functions\n",
        "# ===============================\n",
        "def get_file_id_from_path(file_path):\n",
        "    \"\"\"Extract Google Drive file ID from shareable link or direct path\"\"\"\n",
        "    if '/d/' in file_path:\n",
        "        return file_path.split('/d/')[1].split('/')[0]\n",
        "    return file_path\n",
        "\n",
        "def download_file_from_drive(file_id, local_filename):\n",
        "    \"\"\"Download file from Google Drive\"\"\"\n",
        "    request = drive_service.files().get_media(fileId=file_id)\n",
        "    fh = io.BytesIO()\n",
        "    downloader = MediaIoBaseDownload(fh, request)\n",
        "    done = False\n",
        "    while not done:\n",
        "        status, done = downloader.next_chunk()\n",
        "    fh.seek(0)\n",
        "    with open(local_filename, 'wb') as f:\n",
        "        f.write(fh.read())\n",
        "    print(f\"‚úÖ Downloaded: {local_filename}\")\n",
        "\n",
        "def upload_file_to_custom_location(local_filename, parent_folder_id=None):\n",
        "    \"\"\"Upload the output file to a chosen location in Google Drive\"\"\"\n",
        "    file_metadata = {'name': local_filename}\n",
        "    if parent_folder_id:\n",
        "        file_metadata['parents'] = [parent_folder_id]\n",
        "    media = MediaFileUpload(local_filename, resumable=True)\n",
        "    uploaded_file = drive_service.files().create(\n",
        "        body=file_metadata,\n",
        "        media_body=media,\n",
        "        fields='id, name, webViewLink'\n",
        "    ).execute()\n",
        "    print(f\"\\n‚úÖ Uploaded: {uploaded_file.get('name')}\")\n",
        "    print(f\"üîó View file: {uploaded_file.get('webViewLink')}\")\n",
        "    return uploaded_file.get('id')\n",
        "\n",
        "def check_question_with_perplexity(utterance):\n",
        "    \"\"\"Use Perplexity API to determine if utterance is a question\"\"\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {PERPLEXITY_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "    }\n",
        "    prompt = f\"\"\"Analyze this utterance and determine if it's a question.\n",
        "Utterance: \"{utterance}\"\n",
        "\n",
        "Reply with ONLY \"1\" if it's a question, or \"0\" if it's not a question. No explanation needed.\"\"\"\n",
        "    payload = {\n",
        "        \"model\": \"sonar-pro\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "    }\n",
        "    try:\n",
        "        response = requests.post(PERPLEXITY_URL, json=payload, headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            answer = result['choices'][0]['message']['content'].strip()\n",
        "            return 1 if '1' in answer else 0\n",
        "        else:\n",
        "            print(f\"Error: {response.status_code}\")\n",
        "            return 0\n",
        "    except Exception as e:\n",
        "        print(f\"Exception: {e}\")\n",
        "        return 0\n",
        "\n",
        "# ===============================\n",
        "# üîπ Main processing\n",
        "# ===============================\n",
        "# Input: Provide Google Drive file ID or shareable link\n",
        "INPUT_FILE_PATH = input(\"15BE0sy2lvOwZ_SXvdEjBVBEyqqDn2yRK\").strip()\n",
        "LOCAL_INPUT_FILE = \"input_transcript.csv\"\n",
        "\n",
        "# Extract file ID\n",
        "file_id = get_file_id_from_path(INPUT_FILE_PATH)\n",
        "\n",
        "# Download file\n",
        "download_file_from_drive(file_id, LOCAL_INPUT_FILE)\n",
        "\n",
        "# Read CSV or Excel\n",
        "try:\n",
        "    df = pd.read_csv(LOCAL_INPUT_FILE)\n",
        "except:\n",
        "    df = pd.read_excel(LOCAL_INPUT_FILE)\n",
        "\n",
        "print(f\"\\nProcessing {len(df)} rows...\")\n",
        "\n",
        "# Add column\n",
        "df['is_question'] = 0\n",
        "\n",
        "# Process each utterance\n",
        "for idx, row in df.iterrows():\n",
        "    utterance = str(row.get('utterance', '')).strip()\n",
        "    if utterance and utterance.lower() != 'nan':\n",
        "        is_question = check_question_with_perplexity(utterance)\n",
        "        df.at[idx, 'is_question'] = is_question\n",
        "        print(f\"Row {idx+1}/{len(df)} ‚Üí {is_question}\")\n",
        "        time.sleep(0.5)  # Rate limit\n",
        "\n",
        "# Save output with timestamp\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "LOCAL_OUTPUT_FILE = f\"output_transcript_{timestamp}.csv\"\n",
        "df.to_csv(LOCAL_OUTPUT_FILE, index=False)\n",
        "\n",
        "print(\"\\n‚úÖ Processing complete.\")\n",
        "\n",
        "# Ask where to upload\n",
        "chosen_folder = input(\"106uHJyfpf9-gUQSGS9_C6xHSssWiKgMk\").strip()\n",
        "if '/folders/' in chosen_folder:\n",
        "    folder_id = chosen_folder.split('/folders/')[1].split('/')[0]\n",
        "elif chosen_folder:\n",
        "    folder_id = chosen_folder\n",
        "else:\n",
        "    folder_id = None\n",
        "\n",
        "# Upload result\n",
        "upload_file_to_custom_location(LOCAL_OUTPUT_FILE, folder_id)\n",
        "print(\"\\n‚úÖ Done! File uploaded successfully.\")\n"
      ],
      "metadata": {
        "id": "XW9lvrgoxFmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 2: Personalized NLP for Identifying Question"
      ],
      "metadata": {
        "id": "s6ySqWC_x7BL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets pandas --quiet\n"
      ],
      "metadata": {
        "id": "LbFxkTs4x7rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # Select and upload Training Data Set\n"
      ],
      "metadata": {
        "id": "vpYd6kTm1qI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load CSV, ensure columns are 'utterance' and 'is_question'\n",
        "df = pd.read_csv('utterances_is_question_17k_hybrid.csv')\n",
        "# Preview your data\n",
        "print(df.head())\n",
        "\n",
        "# Rename columns for model input\n",
        "df['text'] = df['utterance']\n",
        "df['label'] = df['is_question'].astype(int)\n",
        "dataset = df[['text', 'label']]\n",
        "dataset.to_json('finetune_data.jsonl', orient='records', lines=True)\n"
      ],
      "metadata": {
        "id": "AFI2XAIc1vxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "hf_dataset = load_dataset('json', data_files='finetune_data.jsonl', split='train')\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "hf_dataset = hf_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"finetuned_distilbert\",\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=2,   # Adjust as needed\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    logging_steps=10,\n",
        "    learning_rate=2e-5,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "LjzVwe581y6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=hf_dataset,\n",
        "    eval_dataset=hf_dataset,\n",
        ")\n",
        "trainer.train()\n",
        "trainer.save_model(\"finetuned_distilbert\") #save model"
      ],
      "metadata": {
        "id": "Tf3_jUaz1_h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir('.'))\n",
        "print(os.listdir('finetuned_distilbert'))"
      ],
      "metadata": {
        "id": "D03ZqrA92F--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers safetensors"
      ],
      "metadata": {
        "id": "6LExY9lW2LV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Load tokenizer from original pretrained model repo (e.g., DistilBERT)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Load fine-tuned model from local folder\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"./finetuned_distilbert\")\n",
        "\n",
        "# ===============================\n",
        "# üîπ Question Detector with Custom DistilBERT Model\n",
        "# üîπ Save output to a chosen Drive folder\n",
        "# ===============================\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload\n",
        "import io\n",
        "import datetime\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Authenticate Google Drive\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "# ===============================\n",
        "# üîπ Helper functions\n",
        "# ===============================\n",
        "def get_file_id_from_path(file_path):\n",
        "    if '/d/' in file_path:\n",
        "        return file_path.split('/d/')[1].split('/')[0]\n",
        "    return file_path\n",
        "\n",
        "def download_file_from_drive(file_id, local_filename):\n",
        "    request = drive_service.files().get_media(fileId=file_id)\n",
        "    fh = io.BytesIO()\n",
        "    downloader = MediaIoBaseDownload(fh, request)\n",
        "    done = False\n",
        "    while not done:\n",
        "        status, done = downloader.next_chunk()\n",
        "    fh.seek(0)\n",
        "    with open(local_filename, 'wb') as f:\n",
        "        f.write(fh.read())\n",
        "    print(f\"‚úÖ Downloaded: {local_filename}\")\n",
        "\n",
        "def upload_file_to_custom_location(local_filename, parent_folder_id=None):\n",
        "    file_metadata = {'name': local_filename}\n",
        "    if parent_folder_id:\n",
        "        file_metadata['parents'] = [parent_folder_id]\n",
        "    media = MediaFileUpload(local_filename, resumable=True)\n",
        "    uploaded_file = drive_service.files().create(\n",
        "        body=file_metadata,\n",
        "        media_body=media,\n",
        "        fields='id, name, webViewLink'\n",
        "    ).execute()\n",
        "    print(f\"\\n‚úÖ Uploaded: {uploaded_file.get('name')}\")\n",
        "    print(f\"üîó View file: {uploaded_file.get('webViewLink')}\")\n",
        "    return uploaded_file.get('id')\n",
        "\n",
        "# === LOAD YOUR MODEL ONCE ===\n",
        "MODEL_PATH = \"finetuned_distilbert\"  # Change if your folder name is different\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Load fine-tuned model from local folder\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"./finetuned_distilbert\")\n",
        "\n",
        "def check_question_with_local_model(utterance):\n",
        "    inputs = tokenizer(str(utterance), return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        predicted_class_id = logits.argmax(dim=1).item()\n",
        "    return predicted_class_id  # 1 = question, 0 = not-question\n",
        "\n",
        "# ===============================\n",
        "# üîπ Main processing\n",
        "# ===============================\n",
        "INPUT_FILE_PATH = input(\"Enter Sheet ID or file link: \").strip()\n",
        "LOCAL_INPUT_FILE = \"input_transcript.csv\"\n",
        "\n",
        "file_id = get_file_id_from_path(INPUT_FILE_PATH)\n",
        "download_file_from_drive(file_id, LOCAL_INPUT_FILE)\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(LOCAL_INPUT_FILE)\n",
        "except:\n",
        "    df = pd.read_excel(LOCAL_INPUT_FILE)\n",
        "\n",
        "print(f\"\\nProcessing {len(df)} rows...\")\n",
        "\n",
        "df[\"is_question\"] = 0\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    utterance = str(row.get(\"utterance\", \"\")).strip()\n",
        "    if utterance and utterance.lower() != \"nan\":\n",
        "        is_question = check_question_with_local_model(utterance)\n",
        "        df.at[idx, \"is_question\"] = is_question\n",
        "        print(f\"Row {idx+1}/{len(df)} ‚Üí {is_question}\")\n",
        "\n",
        "# Save output with timestamp\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "LOCAL_OUTPUT_FILE = f\"output_transcript_{timestamp}.csv\"\n",
        "df.to_csv(LOCAL_OUTPUT_FILE, index=False)\n",
        "\n",
        "print(\"\\n‚úÖ Processing complete.\")\n",
        "\n",
        "chosen_folder = input(\"Enter destination folder ID or link: \").strip()\n",
        "if \"/folders/\" in chosen_folder:\n",
        "    folder_id = chosen_folder.split(\"/folders/\")[1].split(\"/\")[0]\n",
        "elif chosen_folder:\n",
        "    folder_id = chosen_folder\n",
        "else:\n",
        "    folder_id = None\n",
        "\n",
        "upload_file_to_custom_location(LOCAL_OUTPUT_FILE, folder_id)\n",
        "print(\"\\n‚úÖ Done! File uploaded successfully.\")\n"
      ],
      "metadata": {
        "id": "OYwSsXbb2O-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ii) Metric Calculation"
      ],
      "metadata": {
        "id": "n5VpFc7Os2n_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A) \"Student_Talk_Rate\", \"Dialogue_Frequency\", \"Student_Agency\""
      ],
      "metadata": {
        "id": "ly7Ex6avu73x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import timedelta\n",
        "import re\n",
        "\n",
        "# ================================\n",
        "# Helper Functions\n",
        "# ================================\n",
        "\n",
        "def parse_timestamp(ts_str):\n",
        "    \"\"\"Convert MM:SS.SSS to timedelta\"\"\"\n",
        "    minutes, sec_ms = ts_str.split(\":\")\n",
        "    seconds, millis = sec_ms.split(\".\")\n",
        "    return timedelta(minutes=int(minutes), seconds=int(seconds), milliseconds=int(millis))\n",
        "\n",
        "def format_timestamp(td):\n",
        "    total_ms = int(td.total_seconds() * 1000)\n",
        "    minutes = total_ms // 60000\n",
        "    seconds = (total_ms % 60000) // 1000\n",
        "    millis = total_ms % 1000\n",
        "    return f\"{minutes:02}:{seconds:02}.{millis:03}\"\n",
        "\n",
        "# ================================\n",
        "# Metric Calculation\n",
        "# ================================\n",
        "def calculate_metrics(df):\n",
        "    # Convert times\n",
        "    df['start_td'] = df['timestamp'].apply(parse_timestamp)\n",
        "    df['end_td'] = df['end_time'].apply(parse_timestamp)\n",
        "    df['word_count'] = df['utterance'].astype(str).apply(lambda x: len(x.split()))\n",
        "\n",
        "    # Session duration\n",
        "    session_dur = (df['end_td'].max() - df['start_td'].min()).total_seconds() / 60\n",
        "\n",
        "    # Normalize speakers\n",
        "    df['speaker_norm'] = df['speaker'].apply(lambda x: \"Student\" if \"kid\" in x.lower() or \"student\" in x.lower() else \"Teacher\")\n",
        "\n",
        "    # Student Talk Rate\n",
        "    student_words = df.loc[df['speaker_norm']==\"Student\", 'word_count'].sum()\n",
        "    student_talk_rate = student_words / session_dur if session_dur>0 else 0\n",
        "\n",
        "    # Dialogue Frequency\n",
        "    dialogue_count = 0\n",
        "    for i in range(1,len(df)):\n",
        "        if df.loc[i,'speaker_norm'] != df.loc[i-1,'speaker_norm']:\n",
        "            dialogue_count += 1\n",
        "    dialogue_freq = dialogue_count / session_dur if session_dur>0 else 0\n",
        "\n",
        "        # Average Wait Time (teacher Q -> immediate next student response only)\n",
        "    waits = []\n",
        "    for i in range(len(df) - 1):  # up to second last row\n",
        "        row = df.iloc[i]\n",
        "        next_row = df.iloc[i+1]\n",
        "\n",
        "        if row['speaker_norm']==\"Teacher\" and is_question(row['utterance']):\n",
        "            # Only count if immediate next utterance is a student\n",
        "            if next_row['speaker_norm']==\"Student\":\n",
        "                wait = (next_row['start_td'] - row['end_td']).total_seconds()\n",
        "                if wait < 0:\n",
        "                    wait = 0\n",
        "                waits.append(wait)\n",
        "\n",
        "    avg_wait = sum(waits)/len(waits) if waits else None\n",
        "\n",
        "\n",
        "    # Student Agency (initiations)\n",
        "    student_init, teacher_init = 0, 0\n",
        "    for i in range(1,len(df)):\n",
        "        gap = (df.loc[i,'start_td'] - df.loc[i-1]['end_td']).total_seconds()\n",
        "        if df.loc[i,'speaker_norm'] != df.loc[i-1,'speaker_norm'] and gap>=3:\n",
        "            if df.loc[i,'speaker_norm']==\"Student\":\n",
        "                student_init += 1\n",
        "            else:\n",
        "                teacher_init += 1\n",
        "    student_agency = student_init/teacher_init if teacher_init>0 else None\n",
        "\n",
        "    return {\n",
        "        \"Student_Talk_Rate\": student_talk_rate,\n",
        "        \"Dialogue_Frequency\": dialogue_freq,\n",
        "        \"Student_Agency\": student_agency\n",
        "    }\n",
        "\n",
        "# ================================\n",
        "# Process All Files in Directory\n",
        "# ================================\n",
        "def process_directory(input_dir):\n",
        "    all_summaries = []\n",
        "\n",
        "    for filename in os.listdir(input_dir):\n",
        "        if filename.endswith(\".xlsx\") and not filename.startswith(\"~$\"):  # skip temp files\n",
        "            file_path = os.path.join(input_dir, filename)\n",
        "            print(f\"Processing {filename}...\")\n",
        "            df = pd.read_excel(file_path)\n",
        "            metrics = calculate_metrics(df)\n",
        "            summary = pd.DataFrame([metrics])\n",
        "            summary['Chapter_Name'] = df['Chapter_Name'].iloc[0]\n",
        "            all_summaries.append(summary)\n",
        "\n",
        "    if not all_summaries:\n",
        "        print(\"‚ö†Ô∏è No .xlsx files found in directory.\")\n",
        "        return None\n",
        "\n",
        "    final_df = pd.concat(all_summaries, ignore_index=True)\n",
        "\n",
        "    # Reorder columns\n",
        "    cols = [\"Chapter_Name\"] + [c for c in final_df.columns if c!=\"Chapter_Name\"]\n",
        "    final_df = final_df[cols]\n",
        "\n",
        "    # Save combined file\n",
        "    output_path = os.path.join(input_dir, \"All_Chapters_Metrics_RuleBased.xlsx\")\n",
        "    final_df.to_excel(output_path, index=False)\n",
        "    print(f\"‚úÖ Combined metrics saved to: {output_path}\")\n",
        "    return final_df\n",
        "\n",
        "# ================================\n",
        "# Example Usage\n",
        "# ================================\n",
        "input_dir = \"/content/drive/MyDrive/Colab Notebooks/School Project_Reduce Size_Final Code/Data Analysis/Data Transformation/Script Chapter Wise\"\n",
        "\n",
        "result = process_directory(input_dir)\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "UXZSNofhs17x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "B) 'Question_Rate_Student', 'Question_Rate_Teacher'"
      ],
      "metadata": {
        "id": "mRdNo30oxf58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "\n",
        "# Mount your Google Drive in Colab before running this code\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set your input CSV path here\n",
        "csv_path = '/content/drive/MyDrive/Colab Notebooks/School Project_Reduce Size_Final Code/Data Analysis/output_transcript_20251027_131530.csv'\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Parse time stamps for session duration computation\n",
        "def parse_timestamp(ts_str):\n",
        "    minutes, sec_ms = ts_str.split(\":\")\n",
        "    seconds, millis = sec_ms.split(\".\")\n",
        "    return timedelta(minutes=int(minutes), seconds=int(seconds), milliseconds=int(millis))\n",
        "\n",
        "df['start_td'] = df['timestamp'].apply(parse_timestamp)\n",
        "df['end_td'] = df['end_time'].apply(parse_timestamp)\n",
        "\n",
        "# Normalize speaker column for consistent labeling\n",
        "df['speaker_norm'] = df['speaker'].apply(lambda x: 'Student' if 'kid' in x.lower() or 'student' in x.lower() else 'Teacher')\n",
        "\n",
        "# Function to compute question rates for each chapter\n",
        "def compute_qrates(group):\n",
        "    session_dur = (group['end_td'].max() - group['start_td'].min()).total_seconds() / 60\n",
        "    student_questions = group[(group['speaker_norm'] == 'Student') & (group['is_question'] == 1)].shape[0]\n",
        "    teacher_questions = group[(group['speaker_norm'] == 'Teacher') & (group['is_question'] == 1)].shape[0]\n",
        "    student_q_rate = student_questions / session_dur if session_dur > 0 else 0\n",
        "    teacher_q_rate = teacher_questions / session_dur if session_dur > 0 else 0\n",
        "    return pd.Series({\n",
        "        'Question_Rate_Student': student_q_rate,\n",
        "        'Question_Rate_Teacher': teacher_q_rate\n",
        "    })\n",
        "\n",
        "# Group by chapter and compute rates\n",
        "out_df = df.groupby('chapter').apply(compute_qrates).reset_index()\n",
        "\n",
        "# Save to Excel/CSV\n",
        "out_path = '/content/drive/MyDrive/Colab Notebooks/School Project_Reduce Size_Final Code/Data Analysis/Data Transformation/Question_Rates_by_Chapter_metric.xlsx'\n",
        "out_df.to_excel(out_path, index=False)\n",
        "print(f'Saved: {out_path}')\n",
        "print(out_df)\n"
      ],
      "metadata": {
        "id": "3Cxqyo2VgaL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Metric Analysis**"
      ],
      "metadata": {
        "id": "j7js1QMQTCld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A) Categorization"
      ],
      "metadata": {
        "id": "PF5m2DDTTtuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Install and import libraries ---\n",
        "!pip install --quiet gspread gspread_dataframe\n",
        "\n",
        "import pandas as pd\n",
        "import gspread\n",
        "from gspread_dataframe import get_as_dataframe\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "\n",
        "# --- Step 2: Authenticate your Google account ---\n",
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "# --- Step 3: Provide your Google Sheet ID and optional sheet/tab name ---\n",
        "SHEET_ID = \"1sFB3RA9qaPF8ChPuRkzTVMWhdB106ULFj5kH_EPSUzc\"          # üîπ e.g. \"1PhmF8WvpqRgcTBGKpWVznPiFaN_gzT-nPO4XjmSrUbI\"\n",
        "SHEET_NAME = \"Sheet1\"                    # üîπ tab name inside the Google Sheet\n",
        "\n",
        "# --- Step 4: Open and read the sheet ---\n",
        "worksheet = gc.open_by_key(SHEET_ID).worksheet(SHEET_NAME)\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "df = get_as_dataframe(worksheet, evaluate_formulas=True, dtype=str)\n",
        "\n",
        "# --- Step 5: Clean and preview the data ---\n",
        "df = df.dropna(how='all')  # Remove empty rows\n",
        "\n",
        "\n",
        "# --- Logic for labeling ---\n",
        "def mark_content(chapter):\n",
        "    chapter_lower = chapter.lower()\n",
        "    if \"advance\" in chapter_lower:\n",
        "        return \"Advance Module\"        # ‚Üê your own content\n",
        "    elif \"basic\" in chapter_lower:\n",
        "        return \"Traditional Module\"       # ‚Üê traditional content\n",
        "    else:\n",
        "        return \"Unclassified\"\n",
        "\n",
        "# --- Apply the rule ---\n",
        "df[\"Content_Type\"] = df[\"Chapter_Name\"].apply(mark_content)\n",
        "\n",
        "# --- View the result ---\n",
        "print(df)\n",
        "\n"
      ],
      "metadata": {
        "id": "vVgww1J4THWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "B) Mean Valuse Analysis Table"
      ],
      "metadata": {
        "id": "4aZW2FC7TyaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# --- Ensure numeric columns are actually numeric ---\n",
        "numeric_cols = [\n",
        "    \"Student_Talk_Rate\",\n",
        "    \"Dialogue_Frequency\",\n",
        "    \"Student_Agency\",\n",
        "    \"Question_Rate_Student\",\n",
        "    \"Question_Rate_Teacher\"\n",
        "]\n",
        "df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "# --- Compute means by module type ---\n",
        "mean_df = df.groupby(\"Content_Type\")[numeric_cols].mean().round(2)\n",
        "\n",
        "# --- Extract Traditional and Advance means ---\n",
        "try:\n",
        "    basic_means = mean_df.loc[\"Traditional Module\"]\n",
        "    adv_means = mean_df.loc[\"Advance Module\"]\n",
        "except KeyError as e:\n",
        "    raise ValueError(f\"Check your Content_Type values ‚Äî missing one of them. Found: {mean_df.index.tolist()}\")\n",
        "\n",
        "# --- Build comparison table ---\n",
        "table = pd.DataFrame({\n",
        "    \"Metric\": numeric_cols,\n",
        "    \"Traditional Module Mean\": basic_means.values,\n",
        "    \"Advance Module Mean\": adv_means.values\n",
        "})\n",
        "\n",
        "# --- Calculate percentage difference ---\n",
        "table[\"% Difference (‚Üë or ‚Üì)\"] = (\n",
        "    ((table[\"Advance Module Mean\"] - table[\"Traditional Module Mean\"]) / table[\"Traditional Module Mean\"]) * 100\n",
        ").round(0)\n",
        "table[\"% Difference (‚Üë or ‚Üì)\"] = table[\"% Difference (‚Üë or ‚Üì)\"].apply(\n",
        "    lambda x: f\"‚Üë +{int(x)}%\" if x >= 0 else f\"‚Üì {int(x)}%\"\n",
        ")\n",
        "\n",
        "# --- Style output to look like your screenshot ---\n",
        "styled = (\n",
        "    table.style\n",
        "    .format({\n",
        "        \"Traditional Module Mean\": \"{:.2f}\",\n",
        "        \"Advance Module Mean\": \"<b>{:.2f}</b>\",\n",
        "    })\n",
        "    .hide(axis=\"index\")\n",
        "    .set_table_styles([\n",
        "        {\"selector\": \"th\", \"props\": [(\"text-align\", \"left\"), (\"font-weight\", \"bold\"), (\"padding\", \"6px 10px\")]},\n",
        "        {\"selector\": \"td\", \"props\": [(\"text-align\", \"left\"), (\"padding\", \"6px 10px\")]},\n",
        "        {\"selector\": \"caption\", \"props\": [(\"caption-side\", \"top\"), (\"font-size\", \"16px\"), (\"font-weight\", \"bold\")]}\n",
        "    ])\n",
        "    .set_caption(\"üìä 2Ô∏è‚É£ Descriptive Table\")\n",
        ")\n",
        "\n",
        "display(HTML(styled.to_html()))\n"
      ],
      "metadata": {
        "id": "_xtXJKY3Tch4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C) Mean Value Analysis Bar Chart"
      ],
      "metadata": {
        "id": "fcpKxU88UfmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute means for each metric by module type\n",
        "means = df.groupby(\"Content_Type\")[metrics].mean().reset_index()\n",
        "means_melted = means.melt(id_vars=\"Content_Type\", var_name=\"Metric\", value_name=\"Mean_Value\")\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(\n",
        "    x=\"Metric\",\n",
        "    y=\"Mean_Value\",\n",
        "    hue=\"Content_Type\",\n",
        "    data=means_melted,\n",
        "    palette={\"Traditional Module\": \"#A0AEC0\", \"Advance Module\": \"#3182CE\"}\n",
        ")\n",
        "\n",
        "plt.title(\"Advance vs Traditional ‚Äì Engagement Metrics Comparison\", fontsize=13, weight=\"bold\")\n",
        "plt.xlabel(\"Engagement Metric\", fontsize=10)\n",
        "plt.ylabel(\"Mean Value\", fontsize=10)\n",
        "plt.xticks(rotation=20, ha=\"right\")\n",
        "plt.legend(title=\"Module Type\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kJd5pmVHTffr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "D) Light-Weight Direction Tests (Pilot Scale)(Cohen‚Äôs d)"
      ],
      "metadata": {
        "id": "7B1xIH7iUoDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìâ 4Ô∏è‚É£ Light-Weight Direction Tests (Pilot Scale)\n",
        "# =================================================\n",
        "# Instead of running t-tests (which can be unreliable for small sample sizes),\n",
        "# we‚Äôll compute **Cohen‚Äôs d** ‚Äî a standardized effect-size measure that tells us\n",
        "# how strongly two groups differ, expressed in standard-deviation units.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- Define the metrics of interest ---\n",
        "metrics = [\n",
        "    \"Student_Talk_Rate\",\n",
        "    \"Dialogue_Frequency\",\n",
        "    \"Student_Agency\",\n",
        "    \"Question_Rate_Student\",\n",
        "    \"Question_Rate_Teacher\"\n",
        "]\n",
        "\n",
        "# --- Define Cohen‚Äôs d function ---\n",
        "def cohens_d(x, y):\n",
        "    \"\"\"\n",
        "    Computes Cohen‚Äôs d effect size between two independent samples.\n",
        "    Formula: (mean1 ‚àí mean2) / pooled standard deviation\n",
        "    \"\"\"\n",
        "    nx, ny = len(x), len(y)\n",
        "    dof = nx + ny - 2\n",
        "    pooled_std = np.sqrt(((nx - 1) * x.var(ddof=1) + (ny - 1) * y.var(ddof=1)) / dof)\n",
        "    return (x.mean() - y.mean()) / pooled_std\n",
        "\n",
        "# --- Compute and display results ---\n",
        "print(\"üìà Cohen‚Äôs d Effect Sizes (Advance vs Traditional)\\n\")\n",
        "for col in metrics:\n",
        "    x = df[df[\"Content_Type\"] == \"Advance Module\"][col].dropna()\n",
        "    y = df[df[\"Content_Type\"] == \"Traditional Module\"][col].dropna()\n",
        "    d = cohens_d(x, y)\n",
        "    strength = (\n",
        "        \"üü¢ Large\" if abs(d) >= 0.8 else\n",
        "        \"üü† Medium\" if abs(d) >= 0.5 else\n",
        "        \"üü° Small\" if abs(d) >= 0.2 else\n",
        "        \"‚ö™ Negligible\"\n",
        "    )\n",
        "    direction = \"‚Üë Higher in Advance\" if d > 0 else \"‚Üì Higher in Traditional\"\n",
        "    print(f\"{col.replace('_',' '):25s} : d = {d:6.2f}  ‚Üí  {strength} ({direction})\")\n",
        "\n",
        "# --- Interpretation guide ---\n",
        "print(\"\\nüìò Interpretation (rule of thumb):\")\n",
        "print(\"   0.2 = small effect\")\n",
        "print(\"   0.5 = medium effect\")\n",
        "print(\"   0.8 = large effect\\n\")\n",
        "print(\"If most metrics show d ‚â• 0.6 ‚Üí strong practical improvement in pilot terms.\")\n"
      ],
      "metadata": {
        "id": "sAD3uNQvTigL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "E) Directional Robustness Check ‚Äì Mann‚ÄìWhitney U Test"
      ],
      "metadata": {
        "id": "C_h9GgR5U0OG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üß™ 5Ô∏è‚É£ Directional Robustness Check ‚Äì Mann‚ÄìWhitney U Test\n",
        "# =========================================================\n",
        "# Cohen‚Äôs d showed very large differences, but with small pilot samples (‚âà6 per group),\n",
        "# parametric assumptions (normality, equal variance) may not hold.\n",
        "# The Mann‚ÄìWhitney U test is a *non-parametric* alternative that compares\n",
        "# the median ranks between two groups ‚Äî robust even for small n or skewed data.\n",
        "\n",
        "from scipy.stats import mannwhitneyu\n",
        "import pandas as pd\n",
        "\n",
        "# --- Metrics to test ---\n",
        "metrics = [\n",
        "    \"Student_Talk_Rate\",\n",
        "    \"Dialogue_Frequency\",\n",
        "    \"Student_Agency\",\n",
        "    \"Question_Rate_Student\",\n",
        "    \"Question_Rate_Teacher\"\n",
        "]\n",
        "\n",
        "# --- Run one-sided Mann‚ÄìWhitney U tests (Advance > Traditional) ---\n",
        "print(\"üìä Mann‚ÄìWhitney U Test Results  (one-sided: Advance > Traditional)\\n\")\n",
        "\n",
        "results = []\n",
        "for col in metrics:\n",
        "    a = df[df[\"Content_Type\"] == \"Advance Module\"][col].dropna()\n",
        "    b = df[df[\"Content_Type\"] == \"Traditional Module\"][col].dropna()\n",
        "    stat, p = mannwhitneyu(a, b, alternative=\"greater\")\n",
        "    results.append({\"Metric\": col.replace(\"_\", \" \"),\n",
        "                    \"U-Statistic\": stat,\n",
        "                    \"p-Value\": p})\n",
        "    sig = \"‚úÖ directional (p < 0.10)\" if p < 0.10 else \"‚Äì\"\n",
        "    print(f\"{col.replace('_',' '):25s} :  U = {stat:6.2f}   p = {p:6.3f}   {sig}\")\n",
        "\n",
        "# --- Optional: collect into a DataFrame for summary table ---\n",
        "u_df = pd.DataFrame(results).round(3)\n",
        "display(u_df)\n",
        "\n",
        "# --- Interpretation note ---\n",
        "print(\"\\nüìò Interpretation:\")\n",
        "print(\"‚Ä¢ The Mann‚ÄìWhitney U test assesses whether Advance-module values tend to be higher than Traditional.\")\n",
        "print(\"‚Ä¢ p < 0.10 (one-sided) is interpreted here as *directional evidence* of higher engagement in Advance chapters.\")\n",
        "print(\"‚Ä¢ Because n ‚âà 6 per group, this is a robustness check ‚Äî not formal significance testing.\")\n"
      ],
      "metadata": {
        "id": "XBWb-0VTTlel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "F) Engagement Index Analysis"
      ],
      "metadata": {
        "id": "DflFKzIAVHUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üìä 6Ô∏è‚É£ Engagement Index Analysis\n",
        "# ============================================\n",
        "# Combine all five engagement metrics into a single composite \"Engagement Index\"\n",
        "# and test whether Advance Modules show higher overall engagement than Traditional Modules.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.stats import mannwhitneyu\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1Ô∏è‚É£ Define the metrics you already use ---\n",
        "metrics = [\n",
        "    \"Student_Talk_Rate\",\n",
        "    \"Dialogue_Frequency\",\n",
        "    \"Student_Agency\",\n",
        "    \"Question_Rate_Student\",\n",
        "    \"Question_Rate_Teacher\"\n",
        "]\n",
        "\n",
        "# --- 2Ô∏è‚É£ Compute Engagement Index (normalized mean across metrics) ---\n",
        "scaler = MinMaxScaler()\n",
        "df[\"Engagement_Index\"] = scaler.fit_transform(df[metrics]).mean(axis=1)\n",
        "\n",
        "# --- 3Ô∏è‚É£ Visualize boxplot for overall engagement ---\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.boxplot(\n",
        "    x=\"Content_Type\",\n",
        "    y=\"Engagement_Index\",\n",
        "    data=df,\n",
        "    palette={\"Traditional Module\": \"#A0AEC0\", \"Advance Module\": \"#63B3ED\"}\n",
        ")\n",
        "sns.stripplot(\n",
        "    x=\"Content_Type\",\n",
        "    y=\"Engagement_Index\",\n",
        "    data=df,\n",
        "    color=\"black\",\n",
        "    alpha=0.6,\n",
        "    jitter=True,\n",
        "    size=5\n",
        ")\n",
        "plt.title(\"Overall Engagement Index: Advance vs Traditional\", fontsize=13, weight=\"bold\")\n",
        "plt.xlabel(\"Module Type\", fontsize=10)\n",
        "plt.ylabel(\"Engagement Index (0‚Äì1 scale)\", fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- 4Ô∏è‚É£ Compute Cohen‚Äôs d for the composite index ---\n",
        "def cohens_d(x, y):\n",
        "    nx, ny = len(x), len(y)\n",
        "    dof = nx + ny - 2\n",
        "    pooled_std = np.sqrt(((nx - 1)*x.var(ddof=1) + (ny - 1)*y.var(ddof=1)) / dof)\n",
        "    return (x.mean() - y.mean()) / pooled_std\n",
        "\n",
        "adv = df[df[\"Content_Type\"] == \"Advance Module\"][\"Engagement_Index\"]\n",
        "trad = df[df[\"Content_Type\"] == \"Traditional Module\"][\"Engagement_Index\"]\n",
        "\n",
        "d = cohens_d(adv, trad)\n",
        "\n",
        "# --- 5Ô∏è‚É£ Mann‚ÄìWhitney U test (one-sided) ---\n",
        "stat, p = mannwhitneyu(adv, trad, alternative=\"greater\")\n",
        "\n",
        "# --- 6Ô∏è‚É£ Print results with professional interpretation ---\n",
        "print(\"üìà Engagement Index Comparison (Advance vs Traditional)\\n\")\n",
        "print(f\"Cohen‚Äôs d = {d:.2f}\")\n",
        "print(f\"Mann‚ÄìWhitney U = {stat:.2f}, p = {p:.3f}\\n\")\n",
        "\n",
        "if p < 0.10:\n",
        "    print(\"‚úÖ Directional evidence (p < 0.10): Advance Modules show higher overall engagement.\")\n",
        "else:\n",
        "    print(\"‚ö™ No directional difference detected at p < 0.10 level.\")\n",
        "\n",
        "# Interpretation guide\n",
        "print(\"\\nüìò Interpretation:\")\n",
        "print(\"‚Ä¢ Engagement Index combines all five engagement dimensions into a single 0‚Äì1 scale.\")\n",
        "print(\"‚Ä¢ Cohen‚Äôs d shows magnitude of difference (0.2=small, 0.5=medium, 0.8=large).\")\n",
        "print(\"‚Ä¢ Mann‚ÄìWhitney U tests direction ‚Äî whether Advance > Traditional overall.\")\n",
        "print(\"‚Ä¢ Together, they summarize both strength and consistency of engagement improvements.\")\n"
      ],
      "metadata": {
        "id": "Tpol3s_hVJdg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}